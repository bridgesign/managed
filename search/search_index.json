{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Managed: Automate GPU Allocation for PyTorch","text":"<p>Writing code to scale to multi-gpu machines can be a pain. Moreover, it is often the case that you want to run multiple experiments on the same machine, and you want to be able to run them in parallel. All is well and good till its a single GPU but when you have multiple GPUs, you have to manually specify which GPU to use for which experiment. This is where Managed comes in. Managed is a library that allows you to run multiple experiments on the same machine without having to worry about which GPU to use for which experiment.</p> <p>Managed handles the following things for you:</p> <ul> <li>Automatically allocates tensors on the GPUs</li> <li>Transparently moves the tensors from CPU to GPU and vice versa</li> <li>Move tensors between devices (CPU/GPU) according to memory availability</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The easiest way to install Managed is using <code>pip</code>, but you can also install it from source. Managed requires Python 3.6 or higher.</p> pipgit <pre><code>pip install managed\n</code></pre> <pre><code>git clone https://github.com/bridgesign/managed.git\ncd managed\npython setup.py install\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>Managed provides a <code>ManagedTensor</code> class that is a wrapper around the <code>torch.Tensor</code> class. It provides the same API as the <code>torch.Tensor</code> class but in addition uses a <code>DeviceManager</code> object to determince which device to use for the tensor. The <code>DeviceManager</code> object is responsible for allocating tensors on the GPUs and moving tensors between the CPU and the GPUs. It is a singleton object and is shared across all the <code>ManagedTensor</code> objects.</p> <p>Another important class is the <code>ManagedModule</code> class which is a wrapper around the <code>torch.nn.Module</code> class. The class itself can be used to wrap preinitialized <code>torch.nn.Module</code> objects or can be used as a base class for your own modules. Internally it uses the <code>ManagedTensor</code> class to wrap the parameters of the module.</p>"}]}