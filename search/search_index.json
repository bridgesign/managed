{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Managed: Automate GPU Allocation for PyTorch","text":""},{"location":"#overview","title":"Overview","text":"<p>Writing code to scale to multi-gpu machines can be a pain. Moreover, it is often the case that you want to run multiple experiments on the same machine, and you want to be able to run them in parallel. All is well and good till its a single GPU but when you have multiple GPUs, you have to manually specify which GPU to use for which experiment. This is where Managed comes in. Managed is a library that allows you to run multiple experiments on the same machine without having to worry about which GPU to use for which experiment.</p> <p>Managed handles the following things for you:</p> <ul> <li>Automatically allocates tensors on the GPUs</li> <li>Transparently moves the tensors from CPU to GPU and vice versa</li> <li>Move tensors between devices (CPU/GPU) according to memory availability</li> </ul>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Managed provides a <code>ManagedTensor</code> class that is a wrapper around the <code>torch.Tensor</code> class. It provides the same API as the <code>torch.Tensor</code> class but in addition uses a <code>DeviceManager</code> object to determince which device to use for the tensor. The <code>DeviceManager</code> object is responsible for allocating tensors on the GPUs and moving tensors between the CPU and the GPUs. It is a singleton object and is shared across all the <code>ManagedTensor</code> objects.</p> <p>Another important class is the <code>ManagedModule</code> class which is a wrapper around the <code>torch.nn.Module</code> class. The class itself can be used to wrap preinitialized <code>torch.nn.Module</code> objects or can be used as a base class for your own modules. Internally it uses the <code>ManagedTensor</code> class to wrap the parameters of the module.</p> <p>For example, consider the following code:</p> <pre><code>from managed import ManagedTensor as mt\nimport torch\n# Create a tensor on the CPU\nx = torch.randn(2, 3).as_subclass(mt)\n# Create a tensor on the GPU\ny = torch.randn(2, 3).cuda().as_subclass(mt)\n# Add the two tensors\n# The result will be on the GPU\n# x will be moved to the GPU\nz = x + y\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>The recommended way to install <code>managed</code> is via <code>pip</code>:</p> <pre><code>pip install tensor-managed\n</code></pre> <p>Alternatively, you can install <code>managed</code> from source:</p> <pre><code>git clone https://github.com/bridgesign/managed.git\ncd managed\npython setup.py install\n</code></pre>"},{"location":"developer_notes/","title":"Developer Notes","text":""},{"location":"developer_notes/#pinning-issue","title":"Pinning Issue","text":"<p>The pinning issue for model training can be tracked down to the following PyTorch issue - Fix autograd engine checks \u00b7 Issue #65016 \u00b7 pytorch/pytorch (github.com). Even though it is closed, the issue is with the <code>autograd</code> engine and not with the <code>torch</code> library. The modified code even after the PR has the following check:</p> <pre><code>if (grad.device() != metadata.device()) {\n// quick hack for: https://github.com/pytorch/pytorch/issues/65016 but\n// should be eventually removed\nif (!(metadata.is_tensor_subclass() ||\ngrad.unsafeGetTensorImpl()-&gt;is_python_dispatch())) {\nif (grad.dim() == 0) {\ngrad = grad.to(metadata.device());\n} else {\nstd::stringstream ss;\nss &lt;&lt; \"invalid gradient at index \" &lt;&lt; i &lt;&lt; \" - expected device \";\nss &lt;&lt; metadata.device() &lt;&lt; \" but got \" &lt;&lt; grad.device();\nAT_ERROR(format_error(ss.str()));\n}\n}\n}\n</code></pre> <p>To understand how this is an issue, consider the following code:</p> <pre><code>l1 = ManagedModule.from_module(nn.Linear(5,5))\nl2 = ManagedModule.from_module(nn.Linear(5,1))\nin1 = torch.rand(5).as_subclass(ManagedTensor)\nin2 = torch.rand(5).as_subclass(ManagedTensor).cuda()\nloss1 = l2(l1(in1)) # Happens on cpu\nloss2 = l2(l1(in2)) # l1 l2 transferred to gpu\nloss = loss1 +loss2 # loss1 data transferred to gpu\nloss.backward() # error comes from validate_output in engine.cpp\n</code></pre> <p>The graph node construced for the <code>loss1</code> and <code>loss2</code> variables have the <code>metadata.device()</code> to be <code>cpu</code> and <code>cuda:0</code>. The node of <code>loss</code> checks the output gradient device to be <code>cpu</code> and <code>cuda:0</code> and throws an error. The device of the output <code>grad</code> is the same as the one that goes in. All hooks run after the <code>validate_outputs</code> check in <code>engine.cpp</code> and hence there is no way to artificially change the device of the output <code>grad</code> tensor. The only way is to make the condition before check false. So it is not impossible to fix this issue but it is not trivial either.</p> <p>There are ways to go around pinning by not relying on inplace change of device for tensors but then the hopes of optimizing the gradient accumulation are lost. Consider that the computation started on GPU1 and many recurrent steps were done before shifting to GPU2. Now, even though it is possible to accumulate the gradients on GPU2 which is much better, it will have to copy nearly everytime to GPU1. This can happen many times without careful planning. This defeats the purpose of the package and hence for now pinning is the only solution.</p>"},{"location":"developer_notes/#hint-development-and-module-optimizations","title":"Hint development and Module Optimizations","text":"<p>As of now, <code>ManagedModule</code> is working only as an easy wrapping. But there is a lot of scope for optimizations even in the forward pass. It is possible to allocate individual tensors of the module on different GPUs for faster computation. The first and foremost and idea is to use fact that parameters are named and an identifier based learning can be added to 'DeviceManager`. This will be the most basic form of implicit hints. Explicit hints are a long way.</p>"},{"location":"device_manager/","title":"DeviceManager Class Reference","text":"<p>Creates a device manager object. The object is supposed to be used as a context manager. The device manager object is responsible for managing the devices and the tensors. It is unique for each process.</p> <p>It provides an extensive logging interface to log the events. All events are logged with the logger name <code>DEVICE_MANAGER.&lt;id of device manager object&gt;</code>. The logger is configured to log to the console with the default log level set to <code>debug</code>.</p> <p>When <code>managed</code> is imported, a default device manager object is created and used. It is a global object and is used by all the tensors. It can be accessed using <code>managed.device_manager</code>.</p> <pre><code>from managed import device_manager\n# Now it can be directly used\n</code></pre> Source code in <code>managed/device_manager.py</code> <pre><code>class DeviceManager:\n\"\"\"\n    Creates a device manager object. The object is supposed to be used as a context manager.\n    The device manager object is responsible for managing the devices and the tensors.\n    It is unique for each process.\n    It provides an extensive logging interface to log the events. All events are logged with\n    the logger name `DEVICE_MANAGER.&lt;id of device manager object&gt;`. The logger is configured\n    to log to the console with the default log level set to `debug`.\n    When `managed` is imported, a default device manager object is created and used.\n    It is a global object and is used by all the tensors. It can be accessed using\n    `managed.device_manager`.\n    ```python\n    from managed import device_manager\n    # Now it can be directly used\n    ```\n    \"\"\"\ndef __init__(\nself,\ncomm_interface: Any = time,\ncuda_devices: Union[None, Tuple[torch.device, ...]] = None,\nreserved: float = 0.1,\nretry_limit: int = 10,\nwait_time: float = 0.1,\n) -&gt; None:\nif cuda_devices is None:\nself._cuda_devices = tuple(torch.device('cuda:{}'.format(i)) for i in range(torch.cuda.device_count()))\nself._retry_limit = retry_limit\nself._reserved = reserved\nself._wait_time = wait_time\nself._comm_interface = comm_interface\nself._default_level = \"debug\"\nself._log = logging.getLogger(f'DEVICE_MANAGER.{id(self)}')\nhandler = logging.StreamHandler()\nself._log.addHandler(handler)\n@property\ndef default_level(self):\n\"\"\"\n        Default log level for the device manager.\n        Must be one of `debug`, `info`, `warning`, `error`, `critical`.\n        \"\"\"\nreturn self._default_level\n@default_level.setter\ndef default_level(self, value: str):\n\"\"\"\n        Set the default log level for the device manager.\n        \"\"\"\nassert value in LOG_LEVELS\nself._default_level = value\n@property\ndef comm_interface(self):\nreturn self._comm_interface\n@comm_interface.setter\ndef comm_interface(self, value: Any):\nself._comm_interface = value\n@property\ndef cuda_devices(self):\n\"\"\"\n        Tuple of cuda devices to be used.\n        \"\"\"\nreturn self._cuda_devices\n@cuda_devices.setter\ndef cuda_devices(self, value: Tuple[torch.device, ...]):\n\"\"\"\n        Set the cuda devices to be used.\n        \"\"\"\nself._cuda_devices = value\n@property\ndef cpu_device(self):\nreturn torch.device('cpu')\n@property\ndef devices(self):\nreturn self.cpu_device, *self._cuda_devices\n@property\ndef retry_limit(self):\n\"\"\"\n        Number of times to retry before raising an exception.\n        \"\"\"\nreturn self._retry_limit\n@retry_limit.setter\ndef retry_limit(self, value: int):\nself._retry_limit = value\n@property\ndef reserved(self):\n\"\"\"\n        Fraction of the device memory to be reserved.\n        \"\"\"\nreturn self._reserved\n@reserved.setter\ndef reserved(self, value: float):\nassert 0 &lt;= value &lt;= 1\nself._reserved = value\n@property\ndef wait_time(self):\n\"\"\"\n        Time to wait before retrying.\n        \"\"\"\nreturn self._wait_time\n@wait_time.setter\ndef wait_time(self, value: float):\nself._wait_time = value\ndef log(self, msg, level=None):\n\"\"\"\n        Log a message with the given level.\n        Parameters\n        ----------\n        msg : str\n            Message to be logged.\n        level : str, optional\n            Level of the message. Must be one of `debug`, `info`, `warning`, `error`, `critical`.\n            If not provided, the default level is used.\n        Example:\n        ```python\n        from managed import device_manager\n        device_manager.log('This is a debug message', 'debug')\n        device_manager.log('This is an info message', 'info')\n        ```\n        \"\"\"\nif level is None:\nlevel = self._default_level\nif level not in LOG_LEVELS:\nself._log.error(f\"{level} is not a valid logging method - deafulting to {self.default_level}\")\nlevel = self._default_level\ngetattr(self._log, level)(msg)\ndef _find_device(\nself,\ntensor_list: List[_ManagedTensor],\nspace_list: List[int],\nspace_estimate: int = -1,\n) -&gt; torch.device:\n\"\"\"\n        Internal function to find a device for the given tensor list.\n        \"\"\"\nself.log(f'Class of tensor list: {[type(t) for t in tensor_list]}')\nself.log(f'Device of tensor list: {[t.device for t in tensor_list]}')\npinned_device = get_pinned_device(tensor_list)\nself.log(f'Pinned device: {pinned_device}')\nif space_estimate &lt; 0:\nspace_estimate = sum(space_list)\nself.log(f'Space estimate: {space_estimate}')\nif pinned_device is not None:\nif pinned_device == self.cpu_device:\nreturn pinned_device\nif wait_for_avail(\npinned_device,\nspace_estimate,\nself.comm_interface,\nself._wait_time,\nself._retry_limit,\nself._reserved\n):\nreturn pinned_device # If there is a pinned device, return it\nelse:\nraise RuntimeError('Pinned device {} is not available'.format(pinned_device))\n# Logic required to ensure that if we have all tensors on cpu\n# we don't try to send them to a gpu\ncurrent_devices = set()\nfor tensor in tensor_list:\ncurrent_devices.add(tensor.device)\nself.log(f'Current devices: {current_devices}')\n# Also ensures that if we have a single device, we don't try to send\n# to a different device unless we have to. This is to avoid the overhead.\nif len(current_devices) == 1:\ndevice = current_devices.pop()\nif device == self.cpu_device:\nreturn device\nif wait_for_avail(\ndevice,\nspace_estimate,\nself.comm_interface,\nself._wait_time,\n1,\nself._reserved\n):\nreturn device\ndevice_coverage = get_device_coverage(self.devices, tensor_list, space_list)\ndevice_coverage.pop(self.cpu_device, None)\nself.log(f'Device coverage: {device_coverage}')\nsorted_devices = sorted(device_coverage.keys(), key=lambda x: device_coverage[x], reverse=True)\nfor device in sorted_devices:\nif wait_for_avail(\ndevice,\nspace_estimate,\nself.comm_interface,\nself._wait_time,\nself._retry_limit,\nself._reserved\n):\nreturn device\nself.log(f'No device found, returning CPU')\nreturn self.cpu_device # If no device can fit the tensor, return the CPU\ndef send(\nself,\ntensor_list: List[_ManagedTensor],\ndevice: Union[None, torch.device, str] = None,\nspace_estimate: int = -1,\n) -&gt; None:\n\"\"\"\n        Send the given tensor list to the given device if possible\n        or find a device that can fit the tensor list and send it there.\n        Parameters\n        ----------\n        tensor_list : List[_ManagedTensor]\n            List of tensors to be sent.\n        device : Union[None, torch.device, str], optional\n            Device to send the tensor list to. If None, then the device is chosen automatically.\n        space_estimate : int, optional\n            Estimated space required to store the tensor list. If not provided, it is calculated.\n            It can be provided to consider the space required to store some future results.\n        Example:\n        ```python\n        from managed import device_manager\n        import torch\n        x = torch.randn(10, 10)\n        device_manager.send([x], 'cuda:0')\n        ```\n        \"\"\"\nif len(self._cuda_devices) == 0:\nreturn\n# If device is None, then we will try to find a device that can fit the object\nif device is not None:\nif isinstance(device, str):\ndevice = torch.device(device)\nif device not in self.devices:\nraise ValueError('Device {} is not available'.format(device))\nelse:\n# TODO: Optimize this\nif len(tensor_list) &lt; 2:\nreturn\nspace_list = get_space_list(tensor_list, USE_HEURISTIC, HEUSRISTIC_FUNCTION)\nself.log(f'Space list: {space_list}')\ndevice = self._find_device(tensor_list, space_list, space_estimate)\nself.log(f'Found device: {device}')\nfor tensor in tensor_list:\nif tensor.device != device:\ntensor.data = tensor.data.to(device, non_blocking=True)\n# If cannot find a device, then it will return CPU device\ndef find_device(\nself,\nspace_estimate: int,\n*args,\ndisperse: bool = False,\n**kwargs,\n):\n\"\"\"\n        Find a device that can fit the given space estimate.\n        Parameters\n        ----------\n        space_estimate : int\n        disperse : bool, optional\n            If True, then the device is selected randomly prioritizing the device with the most available memory.\n            If False, then the device with the least available memory is selected with a higher chance.\n            Default: False\n        \"\"\"\nif not len(self._cuda_devices):\nreturn self.cpu_device\nfor _ in range(self._retry_limit):\nvalid_devices = {}\nfor device in self._cuda_devices:\nif wait_for_avail(\ndevice,\nspace_estimate,\nself.comm_interface,\nself._wait_time,\n1,\nself._reserved\n):\ntotal, allocated, reserved = cuda_memory_properties(device)                    \nvalid_devices[device] = total - allocated - reserved\nself.log(f'Valid devices: {valid_devices}')\nif len(valid_devices) == 0:\ncontinue\nif len(valid_devices) == 1:\nreturn list(valid_devices.keys())[0]\n# Select randomly based on the disperse flag\ndevices = list(valid_devices.keys())\nvalues = list(valid_devices.values())\nnet = sum(values)\nif disperse:\ndevice = random.choices(devices, weights=values)[0]\nelse:\ndevice = random.choices(devices, weights=[net - v for v in values])[0]\nreturn device\nreturn self.cpu_device\ndef _cuda(\nself,\nobj: Union[torch.nn.Module, torch.Tensor],\n*args,\ndisperse: bool = False,\n**kwargs\n) -&gt; torch.device:\ntensor_list = []\naggregate_tensors(tensor_list, obj)        \n# Heuristic object size\nsize_estimate = sum(get_space_list(tensor_list, USE_HEURISTIC, HEUSRISTIC_FUNCTION))\nreturn self.find_device(size_estimate, *args, disperse=disperse, **kwargs)\ndef cuda(\nself,\nobj: Union[torch.nn.Module, torch.Tensor],\n*args,\ndisperse: bool = False,\n**kwargs\n) -&gt; torch.device:\n\"\"\"\n        Finds a device that can fit the given object and returns the device.\n        Parameters\n        ----------\n        obj : Union[torch.nn.Module, torch.Tensor]\n            Object to be sent to a device.\n        disperse : bool, optional\n            If True, then the device is selected randomly prioritizing the device with the most available memory.\n            If False, then the device with the least available memory is selected with a higher chance.\n            Default: False\n        \"\"\"\nif len(self._cuda_devices) == 0:\nreturn torch.device('cpu')\nif len(self._cuda_devices) == 1:\nreturn self._cuda_devices[0]\nif obj.device.type == 'cuda':\nreturn obj.device\nreturn self._cuda(obj, *args, disperse=disperse, **kwargs)\n</code></pre>"},{"location":"device_manager/#managed.device_manager.DeviceManager.cuda_devices","title":"<code>cuda_devices</code>  <code>writable</code> <code>property</code>","text":"<p>Tuple of cuda devices to be used.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.default_level","title":"<code>default_level</code>  <code>writable</code> <code>property</code>","text":"<p>Default log level for the device manager. Must be one of <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.reserved","title":"<code>reserved</code>  <code>writable</code> <code>property</code>","text":"<p>Fraction of the device memory to be reserved.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.retry_limit","title":"<code>retry_limit</code>  <code>writable</code> <code>property</code>","text":"<p>Number of times to retry before raising an exception.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.wait_time","title":"<code>wait_time</code>  <code>writable</code> <code>property</code>","text":"<p>Time to wait before retrying.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.cuda","title":"<code>cuda(obj, *args, disperse=False, **kwargs)</code>","text":"<p>Finds a device that can fit the given object and returns the device.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.cuda--parameters","title":"Parameters","text":"Union[torch.nn.Module, torch.Tensor] <p>Object to be sent to a device.</p> bool, optional <p>If True, then the device is selected randomly prioritizing the device with the most available memory. If False, then the device with the least available memory is selected with a higher chance. Default: False</p> Source code in <code>managed/device_manager.py</code> <pre><code>def cuda(\nself,\nobj: Union[torch.nn.Module, torch.Tensor],\n*args,\ndisperse: bool = False,\n**kwargs\n) -&gt; torch.device:\n\"\"\"\n    Finds a device that can fit the given object and returns the device.\n    Parameters\n    ----------\n    obj : Union[torch.nn.Module, torch.Tensor]\n        Object to be sent to a device.\n    disperse : bool, optional\n        If True, then the device is selected randomly prioritizing the device with the most available memory.\n        If False, then the device with the least available memory is selected with a higher chance.\n        Default: False\n    \"\"\"\nif len(self._cuda_devices) == 0:\nreturn torch.device('cpu')\nif len(self._cuda_devices) == 1:\nreturn self._cuda_devices[0]\nif obj.device.type == 'cuda':\nreturn obj.device\nreturn self._cuda(obj, *args, disperse=disperse, **kwargs)\n</code></pre>"},{"location":"device_manager/#managed.device_manager.DeviceManager.find_device","title":"<code>find_device(space_estimate, *args, disperse=False, **kwargs)</code>","text":"<p>Find a device that can fit the given space estimate.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.find_device--parameters","title":"Parameters","text":"<p>space_estimate : int</p> bool, optional <p>If True, then the device is selected randomly prioritizing the device with the most available memory. If False, then the device with the least available memory is selected with a higher chance. Default: False</p> Source code in <code>managed/device_manager.py</code> <pre><code>def find_device(\nself,\nspace_estimate: int,\n*args,\ndisperse: bool = False,\n**kwargs,\n):\n\"\"\"\n    Find a device that can fit the given space estimate.\n    Parameters\n    ----------\n    space_estimate : int\n    disperse : bool, optional\n        If True, then the device is selected randomly prioritizing the device with the most available memory.\n        If False, then the device with the least available memory is selected with a higher chance.\n        Default: False\n    \"\"\"\nif not len(self._cuda_devices):\nreturn self.cpu_device\nfor _ in range(self._retry_limit):\nvalid_devices = {}\nfor device in self._cuda_devices:\nif wait_for_avail(\ndevice,\nspace_estimate,\nself.comm_interface,\nself._wait_time,\n1,\nself._reserved\n):\ntotal, allocated, reserved = cuda_memory_properties(device)                    \nvalid_devices[device] = total - allocated - reserved\nself.log(f'Valid devices: {valid_devices}')\nif len(valid_devices) == 0:\ncontinue\nif len(valid_devices) == 1:\nreturn list(valid_devices.keys())[0]\n# Select randomly based on the disperse flag\ndevices = list(valid_devices.keys())\nvalues = list(valid_devices.values())\nnet = sum(values)\nif disperse:\ndevice = random.choices(devices, weights=values)[0]\nelse:\ndevice = random.choices(devices, weights=[net - v for v in values])[0]\nreturn device\nreturn self.cpu_device\n</code></pre>"},{"location":"device_manager/#managed.device_manager.DeviceManager.log","title":"<code>log(msg, level=None)</code>","text":"<p>Log a message with the given level.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.log--parameters","title":"Parameters","text":"str <p>Message to be logged.</p> str, optional <p>Level of the message. Must be one of <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>. If not provided, the default level is used.</p> <p>Example: <pre><code>from managed import device_manager\ndevice_manager.log('This is a debug message', 'debug')\ndevice_manager.log('This is an info message', 'info')\n</code></pre></p> Source code in <code>managed/device_manager.py</code> <pre><code>def log(self, msg, level=None):\n\"\"\"\n    Log a message with the given level.\n    Parameters\n    ----------\n    msg : str\n        Message to be logged.\n    level : str, optional\n        Level of the message. Must be one of `debug`, `info`, `warning`, `error`, `critical`.\n        If not provided, the default level is used.\n    Example:\n    ```python\n    from managed import device_manager\n    device_manager.log('This is a debug message', 'debug')\n    device_manager.log('This is an info message', 'info')\n    ```\n    \"\"\"\nif level is None:\nlevel = self._default_level\nif level not in LOG_LEVELS:\nself._log.error(f\"{level} is not a valid logging method - deafulting to {self.default_level}\")\nlevel = self._default_level\ngetattr(self._log, level)(msg)\n</code></pre>"},{"location":"device_manager/#managed.device_manager.DeviceManager.send","title":"<code>send(tensor_list, device=None, space_estimate=-1)</code>","text":"<p>Send the given tensor list to the given device if possible or find a device that can fit the tensor list and send it there.</p>"},{"location":"device_manager/#managed.device_manager.DeviceManager.send--parameters","title":"Parameters","text":"List[_ManagedTensor] <p>List of tensors to be sent.</p> Union[None, torch.device, str], optional <p>Device to send the tensor list to. If None, then the device is chosen automatically.</p> int, optional <p>Estimated space required to store the tensor list. If not provided, it is calculated. It can be provided to consider the space required to store some future results.</p> <p>Example: <pre><code>from managed import device_manager\nimport torch\nx = torch.randn(10, 10)\ndevice_manager.send([x], 'cuda:0')\n</code></pre></p> Source code in <code>managed/device_manager.py</code> <pre><code>def send(\nself,\ntensor_list: List[_ManagedTensor],\ndevice: Union[None, torch.device, str] = None,\nspace_estimate: int = -1,\n) -&gt; None:\n\"\"\"\n    Send the given tensor list to the given device if possible\n    or find a device that can fit the tensor list and send it there.\n    Parameters\n    ----------\n    tensor_list : List[_ManagedTensor]\n        List of tensors to be sent.\n    device : Union[None, torch.device, str], optional\n        Device to send the tensor list to. If None, then the device is chosen automatically.\n    space_estimate : int, optional\n        Estimated space required to store the tensor list. If not provided, it is calculated.\n        It can be provided to consider the space required to store some future results.\n    Example:\n    ```python\n    from managed import device_manager\n    import torch\n    x = torch.randn(10, 10)\n    device_manager.send([x], 'cuda:0')\n    ```\n    \"\"\"\nif len(self._cuda_devices) == 0:\nreturn\n# If device is None, then we will try to find a device that can fit the object\nif device is not None:\nif isinstance(device, str):\ndevice = torch.device(device)\nif device not in self.devices:\nraise ValueError('Device {} is not available'.format(device))\nelse:\n# TODO: Optimize this\nif len(tensor_list) &lt; 2:\nreturn\nspace_list = get_space_list(tensor_list, USE_HEURISTIC, HEUSRISTIC_FUNCTION)\nself.log(f'Space list: {space_list}')\ndevice = self._find_device(tensor_list, space_list, space_estimate)\nself.log(f'Found device: {device}')\nfor tensor in tensor_list:\nif tensor.device != device:\ntensor.data = tensor.data.to(device, non_blocking=True)\n</code></pre>"},{"location":"limitations/","title":"Limitations","text":"<p>Originally while developing the package, it was to use multiple GPU devices to efficiently train a model in multiple simulations. In general, libraries allow allocating a world to a single GPU device. However, there were cases where the requirement of certain worlds would go beyond memory of a single or that it was possible to use the compute of another GPU to get the job done faster. This was the motivation behind the development of this package.</p> <p>Training models was more of an after thought but found that the same tricks can also help scaling up training. However, as seen in the Quickstart section, models or particular layers need to be pinned. Pinning was originally meant to provide simple hints to the <code>DeviceManager</code> to give fine grained control when required without giving up on the dynamic allocation nature of <code>ManagedTensor</code>.</p>"},{"location":"limitations/#pinning","title":"Pinning","text":"<p>In PyTorch <code>autograd</code> there is a device check on the computed gradients which causes a problem as the gradients might be calculated on a completely different device if the <code>tensor</code> in consideration was shifted. This is the base reason why pinning is required. Further explanation can be found in the Depeloper Notes section.</p>"},{"location":"limitations/#device-manager","title":"Device Manager","text":"<p>As of now there is are no ways to create smart hints for the <code>DeviceManager</code> to dynamically optimize the allocation of tensors for repeated calls. This is a work in progress and will be updated as soon as it is available. For now, the user needs to rely on simple pinning and manual device finding (using <code>device_manager.find_device</code>) to get the best out of the package.</p> <p>The <code>DeviceManager</code> relies on a heuristic on the size of the tensor to determine which device to allocate the tensor on. This is a simple heuristic and is not guaranteed to be the best. To disable the heuristic or change the heuristic function:</p> <pre><code>import managed\n# Disable the heuristic\nmanaged.USE_HEURISTIC = False\n# Change the heuristic function : Callable[[int], int]\nmanaged.HEURISTIC_FUNCTION = lambda size: size + int(1.5*size**0.5) # This is the default heuristic function\n</code></pre>"},{"location":"managed_module/","title":"ManagedModule Class Reference","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>A module that uses managed tensors. It can be used to convert a module to use managed tensors or can be used as a base class for a module.</p> Source code in <code>managed/module.py</code> <pre><code>class ManagedModule(torch.nn.Module):\n\"\"\"\n    A module that uses managed tensors. It can be used to convert a module\n    to use managed tensors or can be used as a base class for a module.\n    \"\"\"\n@classmethod\ndef from_module(cls, module: torch.nn.Module, tensor_cls=ManagedTensor):\n\"\"\"\n        Converts a module to use managed tensors.\n        \"\"\"\nif isinstance(module, cls):\nreturn module\nif isinstance(module, torch.nn.Module):\nmodule._apply(wrap_tensor(tensor_cls))\nfor m in module.children():\ncls.from_module(m)\nmodule_class = module.__class__\nnew_cls = type(\nf\"{cls.__name__}.{module_class.__name__}\",\n(module_class, cls),\n{}\n)\nmodule.__class__ = new_cls\nelse:\nraise TypeError(f\"Expected torch Module, got {type(module)}\")\nreturn module\ndef register_parameter(self, name: str, param: Optional[torch.nn.Parameter]) -&gt; None:\nsuper().register_parameter(name, param)\nwrap_tensor(ManagedTensor)(param)\ndef add_module(self, name: str, module: Optional[torch.nn.Module]) -&gt; None:\nsuper().add_module(name, module)\nself.from_module(module)\ndef cuda(self, *args, **kwargs):\nself._apply(lambda t: t.cuda(*args, **kwargs))\nreturn self\ndef apply_(self, fn):\nself._apply(_inplace_func_wrap(fn))\ndef pin(self):\n\"\"\"\n        Pins all the tensors in the module.\n        \"\"\"\nself.apply_(lambda t: t.pin())\ndef unpin(self):\n\"\"\"\n        Unpins all the tensors in the module.\n        \"\"\"\nself.apply_(lambda t: t.unpin())\n</code></pre>"},{"location":"managed_module/#managed.module.ManagedModule.from_module","title":"<code>from_module(module, tensor_cls=ManagedTensor)</code>  <code>classmethod</code>","text":"<p>Converts a module to use managed tensors.</p> Source code in <code>managed/module.py</code> <pre><code>@classmethod\ndef from_module(cls, module: torch.nn.Module, tensor_cls=ManagedTensor):\n\"\"\"\n    Converts a module to use managed tensors.\n    \"\"\"\nif isinstance(module, cls):\nreturn module\nif isinstance(module, torch.nn.Module):\nmodule._apply(wrap_tensor(tensor_cls))\nfor m in module.children():\ncls.from_module(m)\nmodule_class = module.__class__\nnew_cls = type(\nf\"{cls.__name__}.{module_class.__name__}\",\n(module_class, cls),\n{}\n)\nmodule.__class__ = new_cls\nelse:\nraise TypeError(f\"Expected torch Module, got {type(module)}\")\nreturn module\n</code></pre>"},{"location":"managed_module/#managed.module.ManagedModule.pin","title":"<code>pin()</code>","text":"<p>Pins all the tensors in the module.</p> Source code in <code>managed/module.py</code> <pre><code>def pin(self):\n\"\"\"\n    Pins all the tensors in the module.\n    \"\"\"\nself.apply_(lambda t: t.pin())\n</code></pre>"},{"location":"managed_module/#managed.module.ManagedModule.unpin","title":"<code>unpin()</code>","text":"<p>Unpins all the tensors in the module.</p> Source code in <code>managed/module.py</code> <pre><code>def unpin(self):\n\"\"\"\n    Unpins all the tensors in the module.\n    \"\"\"\nself.apply_(lambda t: t.unpin())\n</code></pre>"},{"location":"managed_tensor/","title":"ManagedTensor Class Reference","text":"<p>         Bases: <code>_ManagedTensor</code></p> <p>Managed Tensor class</p> <p>This class is a subclass of torch.Tensor It overrides the torch_function method to send the tensor to the device manager</p>"},{"location":"managed_tensor/#managed.tensor.ManagedTensor--properties","title":"Properties","text":"<p>pinned: bool - True if tensor is pinned to the device</p>"},{"location":"managed_tensor/#managed.tensor.ManagedTensor--methods","title":"Methods","text":"pin <p>Pin the tensor to the device</p> unpin <p>Unpin the tensor from the device</p> cpu <p>Move the tensor to cpu</p> cuda <p>Move the tensor to cuda. If no args are passed, uses the device manager to find a suitable device. If args are passed, uses the first arg as the device to move the tensor to.</p> <p>Device Manager specific args can be directly passed to the function</p> to <p>Move the tensor to the device. If string input cuda is used then use the device manager to find a suitable device.</p> <p>Device Manager specific args can be directly passed to the function.</p> <p>Example: <pre><code>import torch\nfrom managed import ManagedTensor as mt\n# Create a cuda tensor\na = torch.rand(10, 10).as_subclass(mt).cuda()\n# Pin the tensor to gpu device\na.pin()\n# Move the tensor to cpu\n# Now pinned to cpu!\na = a.cpu()\n# Create another cuda tensor\nb = torch.rand(10, 10).as_subclass(mt).cuda()\n# Result is on cpu as a is pinned to cpu\nc = a + b\n</code></pre></p> <p>ManagedTensor can be mixed with normal torch.Tensor</p> <p>Though it is recommended to use ManagedTensor for all tensors as normal tensor will be changed to _ManagedTensor in future.</p> <p>Example: <pre><code>a = torch.rand(10, 10).as_subclass(mt).cuda()\nb = torch.rand(10, 10)\n# Class of b will be changed to _ManagedTensor\n# Result and b will be on cuda device of a\nc = a + b\n</code></pre></p> Source code in <code>managed/tensor.py</code> <pre><code>class ManagedTensor(_ManagedTensor):\n\"\"\" Managed Tensor class\n    This class is a subclass of torch.Tensor\n    It overrides the __torch_function__ method\n    to send the tensor to the device manager\n    ## Properties\n    pinned: bool - True if tensor is pinned to the device\n    ## Methods\n    pin:\n        Pin the tensor to the device\n    unpin:\n        Unpin the tensor from the device\n    cpu:\n        Move the tensor to cpu\n    cuda:\n        Move the tensor to cuda. If no args are passed, uses the device manager\n        to find a suitable device. If args are passed, uses the first arg as\n        the device to move the tensor to.\n        Device Manager specific args can be directly passed to the function\n    to:\n        Move the tensor to the device. If string input cuda is used then use the\n        device manager to find a suitable device.\n        Device Manager specific args can be directly passed to the function.\n    Example:\n    ```python\n    import torch\n    from managed import ManagedTensor as mt\n    # Create a cuda tensor\n    a = torch.rand(10, 10).as_subclass(mt).cuda()\n    # Pin the tensor to gpu device\n    a.pin()\n    # Move the tensor to cpu\n    # Now pinned to cpu!\n    a = a.cpu()\n    # Create another cuda tensor\n    b = torch.rand(10, 10).as_subclass(mt).cuda()\n    # Result is on cpu as a is pinned to cpu\n    c = a + b\n    ```\n    ManagedTensor can be mixed with normal torch.Tensor\n    Though it is recommended to use ManagedTensor for all tensors\n    as normal tensor will be changed to _ManagedTensor in future.\n    Example:\n    ```python\n    a = torch.rand(10, 10).as_subclass(mt).cuda()\n    b = torch.rand(10, 10)\n    # Class of b will be changed to _ManagedTensor\n    # Result and b will be on cuda device of a\n    c = a + b\n    ```\n    \"\"\"\n@classmethod\ndef __torch_function__(cls, func, types, args=[], kwargs=None):\nif kwargs is None:\nkwargs = {}\n# TODO: This needs to be optimized\ntensor_list = []\nif func.__name__ not in FUNC_BLACKLIST:\naggregate_tensors(tensor_list, args)\naggregate_tensors(tensor_list, kwargs)\ndevice_manager.send(tensor_list)\n############################################\n# TODO: This is a temporary fix for\n# device type check from pytroch but it is not\n# fool-proof. Rest require manual pinning of module\n# Issue: https://github.com/pytorch/pytorch/issues/65016\n# Remove this when issue is fixed properly\n############################################\nret = super().__torch_function__(func, types, args, kwargs)\nif func.__name__ not in FUNC_BLACKLIST and func.__name__ != \"backward\":\nret_list = []\naggregate_tensors(ret_list, ret)\nif len(ret_list) == 0:\nreturn ret\nfor t in ret_list:\nhook_unexplored_graph(t.grad_fn, t.device)\nreturn ret\ndef cuda(self, *args, **kwargs):\nif len(args) &gt; 0:\nif isinstance(args[0], int):\nreturn super().cuda(*args, **kwargs).as_subclass(self.__class__)\nelif args[0] is None:\ndevice = device_manager.cuda(self, *args, **kwargs)\nelif isinstance(args[0], torch.device):\ndevice = args[0]\nelse:\nraise TypeError(\"Given device is not a cuda index or torch device\")\nelif 'device' in kwargs:\ndevice = kwargs['device']\nelse:\ndevice = device_manager.cuda(self, *args, **kwargs)\nreturn super().to(device).as_subclass(self.__class__)\n</code></pre>"},{"location":"quick_start/","title":"Quickstart","text":"<p>As the main aim of <code>managed</code> is to work with multiple GPUs, we will assume that you have access to a machine with multiple GPUs. Suppose you have a tessalated surface and want to solve a PDE on it. Now, the PDE can be shown to be equivalent to a linear system of equations. Consider the simple single GPU code below:</p> <pre><code>import torch\nfrom concurrent.futures import ThreadPoolExecutor\n# CPU Bound\ndef get_neighbors(x):\n\"\"\"\n    Returns the neighbors of each element in x.\n    \"\"\"\n....\nreturn neighbors\n# CPU Bound\ndef construct_pde_matrix(x, neighbors):\n\"\"\"\n    Constructs the PDE matrix for the given x.\n    \"\"\"\n...\nreturn pde_matrix\n# Can be put on GPU!!\ndef solve_pde(x):\n\"\"\"\n    Solves the PDE for the given x.\n    \"\"\"\nneighbors = get_neighbors(x)\n# Goes on the default cuda device\npde_matrix = construct_pde_matrix(x, neighbors).cuda()\n# Same - default cuda device\nx = x.cuda() \nsolution = torch.linalg.solve(pde_matrix, x)\n...\nreturn solution\n# CPU Bound\nexecutor = ThreadPoolExecutor(max_workers=4)\nfor _ in range(ITERATIONS):\nfutures = []\nfor point in tessalation:\nfutures.append(executor.submit(solve_pde, point))\nfor future in futures:\nret = future.result()\n# Do something with ret\n</code></pre> <p>Here there are many cpu bound operations like finding the neighbors and creating the system. However, the main bottleneck is solving the PDE. To use multiple GPUs, you will have to add extra logic on where the allocation must happen. Some systems might take more time and might keep the GPU busy for longer. A simple polling based approach will only give you a marginal improvement. In addition, if there is a spill over at some point, it will have to be dealt separately.</p> <p>With <code>managed</code>, you can write the same code as:</p> <pre><code>from managed import ManagedTensor as mt\n# Can be put on GPU!!\ndef solve_pde(x):\n\"\"\"\n    Solves the PDE for the given x.\n    \"\"\"\nneighbors = get_neighbors(x)\n# Goes to GPU device with least memory usage\npde_matrix = construct_pde_matrix(x, neighbors).as_subclass(mt).cuda(disperse=True)\n# Same device as pde_matrix\nx = x.as_subclass(mt).cuda(pde_matrix.device)\n# That's it! No need to change anything else\nsolution = torch.linalg.solve(pde_matrix, x)\n...\nreturn solution\n</code></pre> <p>There are two things to note here - the <code>disperse</code> flag and that we give a device for <code>x</code>. The reason we give a device to <code>x</code> is because it is going to be shifted to the GPU with PDE matrices. If we had not done this, there was a chance that <code>x</code> would be allocated on a different device and then we would have to again move it. The <code>disperse</code> flag tells <code>managed</code> to disperse the tensors across the GPUs. This is done by the <code>DeviceManager</code> object. The <code>DeviceManager</code> object is a singleton object and is shared across all the <code>ManagedTensor</code> objects. It is responsible for allocating tensors on the GPUs and moving tensors between the CPU and the GPUs. It also moves tensors between devices (CPU/GPU) according to memory availability.</p> <p>Now let us consider the case where we want to train multiple small models. However, the convergence of the models is not uniform. Some models might converge faster than others. In such a case, again we will face a similar issue as above. We will have to add extra logic to allocate the models to the GPUs. With <code>managed</code>, you can write the code as:</p> <pre><code>import torch\nfrom managed import ManagedModule as mm\nfrom concurrent.futures import ThreadPoolExecutor\n# Inherit ManagedModule instead of torch.nn.Module\nclass MyModel(mm):\n\"\"\"\n    A simple model that takes in a tensor and returns the sum of its elements.\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.linear = torch.nn.Linear(10, 1)\ndef forward(self, x):\nreturn self.linear(x).sum()\ndef run_experiment(trainloader):\n\"\"\"\n    Runs an experiment on the given trainloader.\n    \"\"\"\nmodel = MyModel().cuda()\nmodel.pin() # Pin the model - this is important!\noptimizer = torch.optim.Adam(model.parameters())\nwhile True:\nnet_loss = 0\nfor x, y in trainloader:\noptimizer.zero_grad()\nloss = model(x) - y\nnet_loss += loss.item()\nloss.backward()\noptimizer.step()\nif net_loss &lt; 1e-3:\nbreak\nreturn model\nif __name__ == \"__main__\":\nexecutor = ThreadPoolExecutor(max_workers=4)\ntrainloader = torch.utils.data.DataLoader(torch.randn(100, 10), batch_size=10)\nfutures = []\nfor _ in range(100):\nfutures.append(executor.submit(run_experiment, trainloader))\nfor future in futures:\nmodel = future.result()\nprint(model(torch.randn(10)))\n</code></pre> <p>Here we have used the <code>ManagedModule</code> class which is a wrapper around the <code>torch.nn.Module</code> class. In case you do not want to inherit <code>ManagedModule</code>, you can use the <code>ManagedModule.from_module</code> method to wrap a preinitialized <code>torch.nn.Module</code> object.</p> <pre><code>model = mm.from_module(model)\n</code></pre> <p>In the above code, we pin the model. Pinning is a property of <code>ManagedTensor</code> objects. When a tensor is pinned, it is not moved between devices. This is useful when you want to keep a tensor on a particular device. Pinning a model is same as pinning all the parameters of the model. The exact reasons why we need to pin the model are explained in the Limitations section. Note that we do not need to pin the model in the previous example as it is a simple linear model. However, there are cases like when the model has a recurrent layer we need to pin the recurrent layer not necessarily the whole model. Pinning the whole model is a safe option but it might not be the most efficient option.</p> <p>Pinning is required only when training the model. It is not required when we are not relying on the autogard gradient calculation.</p>"}]}